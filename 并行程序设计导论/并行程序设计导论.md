# 并行计算

## 1.cpu基本概念

• socket：主板上实际插入的 cpu 硬件个数

• core：cpu的核心数，一开始，每个物理 cpu 上只有一个核心（a single core），对操作系统而言，也就是同一时刻只能运行一个进程/线程。 为了提高性能，cpu 厂商开始在单个物理 cpu 上增加核心（实实在在的硬件存在），也就出现了双核心 cpu（dual-core cpu）以及多核心 cpu（multiple cores），这样一个双核心 cpu 就是同一时刻能够运行两个进程/线程的。后来，出现了多线程技术（simultaneous multithreading，SMT，是AMD和其他CPU厂商的称呼）和超线程技术（hyper–threading，HT，Intel称呼，可以认为是SMT的一种具体技术实现）为了提高单个core同一时刻能够执行的多线程数的技术

• 查看 cpu 信息：linux 系统，lscpu

• 总的逻辑 cpu 数 = 物理 cpu 数 * 每颗物理 cpu 的核心数 * 每个核心的超线程数

## 并行计算相关概念

• 并行（parallel）、分布式（distributed）、并发（cocurrent）的区别：并发计算中，一个程序的多个任务在同一个时间段内可以同时进行，并行计算中一个程序的多个任务紧密协作来解决某个问题，更多的应用场景是多核，分布式计算中一个程序需要与其他程序协作来解决某个问题，更多的应用场景是多计算机，并行和分布式都是并发的。

• 并行计算：并行计算是相对于串行计算来说的。可分为时间上的并行和空间上的并行。 时间上的并行就是指流水线技术，而空间上的并行则是指用多个处理器并发的执行计算。并行计算的目的就是提供单处理器无法提供的性能（处理器能力或存储器），使用多处理器求解单个问题。 

• 分布式计算：分布式计算研究如何把一个需要非常巨大的计算能力才能解决的问题分成许多小的部分，然后把这些部分分配给许多计算机进行处理，最后把这些计算结果综合起来得到最终的结果。最近的分布式计算项目已经被用于使用世界各地成千上万位志愿者的计算机的闲置计算能力，通过因特网，可以分析来自外太空的电讯号，寻找隐蔽的黑洞，并探索可能存在的外星智慧生命等。 

• 并行计算与分布式计算的区别：（1）简单的理解，并行计算借助并行算法和并行编程语言能够实现进程级并行（如MPI）和线程级并行（如openMP）。而分布式计算只是将任务分成小块到各个计算机分别计算各自执行。（2）粒度方面，并行计算中，处理器间的交互一般很频繁，往往具有细粒度和低开销的特征，并且被认为是可靠的。而在分布式计算中，处理器间的交互不频繁，交互特征是粗粒度，并且被认为是不可靠的。并行计算注重短的执行时间，分布式计算则注重长的正常运行时间。（3）联系，并行计算和分布式计算两者是密切相关的。某些特征与程度（处理器间交互频率）有关，而我们还未对这种交叉点（crossover point）进行解释。另一些特征则与侧重点有关（速度与可靠性），而且我们知道这两个特性对并行和分布两类系统都很重要。（4）总之，这两种不同类型的计算在一个多维空间中代表不同但又相邻的点。 

• 集群计算：计算机集群使将一组松散集成的计算机软件和/或硬件连接起来高度紧密地协作完成计算工作。在某种意义上，他们可以被看作是一台计算机。集群系统中的单个计算机通常称为节点，通常通过局域网连接，但也有其它的可能连接方式。集群计算机通常用来改进单个计算机的计算速度和/或可靠性。一般情况下集群计算机比单个计算机，比如工作站或超级计算机性价比要高得多。根据组成集群系统的计算机之间体系结构是否相同，集群可分为同构与异构两种。集群计算机按功能和结构可以分为，高可用性集群（High-availability (HA) clusters）、负载均衡集群（Loadbalancing clusters）、高性能计算集群（High-performance (HPC)clusters）、网格计算（Grid computing）。 高可用性集群，一般是指当集群中有某个节点失效的情况下，其上的任务会自动转移到其他正常的节点上。还指可以将集群中的某节点进行离线维护再上线，该过程并不影响整个集群的运行。 

• 负载均衡集群，负载均衡集群运行时，一般通过一个或者多个前端负载均衡器，将工作负载分发到后端的一组服务器上，从而达到整个系统的高性能和高可用性。这样的计算机集群有时也被称为服务器群（Server Farm）。一般高可用性集群和负载均衡集群会使用类似的技术，或同时具有高可用性与负载均衡的特点。Linux虚拟服务器（LVS）项目在Linux操作系统上提供了最常用的负载均衡软件。 高性能计算集群，高性能计算集群采用将计算任务分配到集群的不同计算节点儿提高计算能力，因而主要应用在科学计算领域。比较流行的HPC采用Linux操作系统和其它一些免费软件来完成并行运算。这一集群配置通常被称为Beowulf集群。这类集群通常运行特定的程序以发挥HPC cluster的并行能力。这类程序一般应用特定的运行库, 比如专为科学计算设计的MPI库。HPC集群特别适合于在计算中各计算节点之间发生大量数据通讯的计算作业，比如一个节点的中间结果或影响到其它节点计算结果的情况。 

• 网格计算：网格计算是分布式计算的一种，也是一种与集群计算非常相关的技术。如果我们说某项工作是分布式的，那么，参与这项工作的一定不只是一台计算机，而是一个计算机网络，显然这种“蚂蚁搬山”的方式将具有很强的数据处理能力。网格计算的实质就是组合与共享资源并确保系统安全。网格计算通过利用大量异构计算机的未用资源（CPU周期和磁盘存储），将其作为嵌入在分布式电信基础设施中的一个虚拟的计算机集群，为解决大规模的计算问题提供一个模型。网格计算的焦点放在支持跨管理域计算的能力，这使它与传统的计算机集群或传统的分布式计算相区别。网格计算的目标是解决对于任何单一的超级计算机来说仍然大得难以解决的问题，并同时保持解决多个较小的问题的灵活性。这样，网格计算就提供了一个多用户环境。
  
## 2.并行硬件和软件

### 串行系统

• 冯诺依曼结构：计算机硬件的标准模型是冯诺依曼结构，由执行计算的CPU（CPU分为控制单元和算术逻辑单元ALU，CPU中的数据和程序执行时的状态存储在寄存器中），主存组成，CPU和主存之间靠总线互连。CPU与主存的分离成为冯诺依曼瓶颈。

• cache：为了突破冯诺依曼瓶颈，cahe，即CPU缓存是在CPU寄存器与主存之间的中间存储器，主要目的是降低主存访问的延迟。

• 指令集并行（ILP）使单进程可以同时执行多个指令，主要有两种ILP：流水线和多发射，对于流水线，处理器的功能单元被依次排列，其中一个的作为另一个的输入。当一个数据在第二个处理单元内成立时，另一个数据就能在第一个处理单元内处理。对于多发射，同类型的功能单元会被复制，处理器可以同时在单个程序执行多条不同的指令。

### 并行硬件

• 指令集并行和线程级并行在底层提供并行性，主要由处理器和操作系统来控制，而不是由程序员直接控制。通常利用Flynn分类法对并行硬件进行分类，冯诺依曼系统拥有单个的指令流和单个的数据流，所以单指令流单数据流（SISD）。

• 单指令流多数据流（SIMD）：在任意时间执行一条指令，但是该指令可以对多个数据项进行操作。

• 多指令流多数据流（MIMD）：系统同时执行多个指令流，每个指令流有自己的数据流。MIMD系统是多个自主处理器的集合，每个处理器可以按照自己的方式运行，不同的MIMD系统的主要区别在于是共享内存系统还是分布式内存系统，大多数MIMD是混合系统，由多个相对小的共享内存系统通过网络连接来实现，单个的共享内存系统有时称为节点。

### 并行软件

• MPI：Message-Passing Interface，消息传递接口，为分布式内存系统的编程设计，是C语言的扩展库，支持C、C++、Fortran。

• Pthreads：POSIX threads，POSIX线程，为共享内存系统的编程设计，是C语言的扩展库

• OpenMP：为共享内存系统的编程设计，包含一个扩展库以及对C编译器的部分修改，OpenMP相对于Pthreads，对C语言有更高层次的扩展，支持C、C++、Fortran。

• CUDA：NVIDIA GPU并行计算。

• Hadoop是由java语言编写的，在分布式服务器集群上存储海量数据并运行分布式分析应用的开源框架，其核心部件是HDFS与MapReduce，HDFS是一个分布式文件系统：引入存放文件元数据信息的服务器Namenode和实际存放数据的服务器Datanode，对数据进行分布式储存和读取。MapReduce是一个计算框架：MapReduce的核心思想是把计算任务分配给集群内的服务器里执行。通过对计算任务的拆分（Map计算/Reduce计算）再根据任务调度器（JobTracker）对任务进行分布式计算。

## 3.MPI进行分布式内存编程

[配置环境](https://blog.csdn.net/renjunyang007/article/details/104970394)：win下，mingw-w64+[Microsoft MPI](https://docs.microsoft.com/en-us/message-passing-interface/microsoft-mpi-release-notes)（一共两个，在cmd里输入set MSMPI可查看安装情况）+vscode(CodeRunner+修改jason文件)  

注意cmd(chcp可查看)的编码和c文件的编码应该一致，此示例为UTF-8

### 3.1 MPI框架的初始化和结束

```js
int MPI_Init(
    #告知MPI所有必要的初始化设置，返回int型错误码，一般情况下可忽略
    #指向argc的指针，不使用设置为NULL
    int *  argc_p /*in/out*/,
    #指向argv的指针，不使用设置为NULL
    char *** argv_p /*in/out*/ )

int MPI_Finalize(void)
#告知MPI并行框架结束
```

### 3.2 MPI_COMM_WORLD通信子

通信子是一组可以互相发送消息的进程集合。MPI_Init的其中一个目的，是在用户启动程序时，定义由用户启动的所有进程组成的通信子。

```js
int MPI_Comm_size(
    #通信子
    MPI_Comm comm /*in*/,
    #返回通信子的进程号
    int*  comm_sz_p /*out*/)

int MPI_Comm_rank(
    #通信子
    MPI_Comm comm /*in*/,
    #返回正在调用进程在通信子的进程号
    int*  my_rank_p /*out*/)
```

### 3.3 发送模式

1.MPI为发送操作提供了四种模式：标准（standard），同步（synchronous），就绪（ready）和缓冲（buffered）。  
2.不同的模式下，MPI实现决定是将消息的内容复制到自己的存储空间，还是一直阻塞到一个相匹配的接收操作被提交。
|MPI数据类型|C语言数据类型|
|:-|-:|
|MPI_CHAR     |signed char|
|MPI_SHOR     |signed short int|
|MPI_INT       |    signed int|
|MPI_LONG       |signed long int|
|MPI_LONG_LONG  |signed long long int|
|MPI_UNSIGNED_CHAR|unsigned char|
|MPI_UNSIGNED_SHORT|unsigned short int|
|MPI_UNSIGNED    |unsigned int|
|MPI_UNSIGNED    |unsigned long int|
|MPI_FLOAT       |float|
|MPI_DOUBLE        |double|
|MPI_LONG_DOUBLE|long double|
|MPI_BYTE||
|MPI_PACKED||

MPI_Send 标准模式  
MPI标准允许以两种不同的方式来实现:
1.简单地将消息复制到MPI设置的缓冲区并返回。
2.直到对应的MPI_Recv出现前都阻塞。
此外，许多MPI函数都设置了使系统从缓冲到阻塞间切换的阀值，即相对较小的消息就交由MPI_Send缓冲，但对于大型数据就选择阻塞模式

```js
int MPI_Send(
    #指向包含消息的内存块的指针，如greeting
    void*  msg_buf_p /*in*/,
    #指定要发送的数据量，如strlen(greeting)+1
    int  msg_size  /*in*/,
    #数据类型MPI
    MPI_Datatype  msg_type /*in*/,
    #指定要接收消息的进程号
    int  dest /*in*/,
    #tag:0用于表示消息要打印的，为1表示消息用于计算的
    int tag /*in*/,
    #通信子
    MPI_Comm  commuicator /*in*/)
```

MPI_Ssend 同步模式，发送操作会一直阻塞到一个相匹配的接收操作被提交  
MPI_Rsend，就绪模式，在发送操作前会有一个相匹配的接收操作被提交，否则发送操作就是错误的。  
MPI_Bsend 缓冲模式，如果一个相关匹配的接收操作还没有被提交，那么MPI必须复制消息到本地存储空间。本地存储空间必须由用户程序提供。

```js
int MPI_Ssend(
    #指向包含消息的内存块的指针，如greeting
    void*  msg_buf_p /*in*/,
    #指定要发送的数据量，如strlen(greeting)+1
    int  msg_size  /*in*/,
    #数据类型MPI
    MPI_Datatype  msg_type /*in*/,
    #指定要接收消息的进程号
    int  dest /*in*/,
    #tag:0用于表示消息要打印的，为1表示消息用于计算的
    int tag /*in*/,
    #通信子
    MPI_Comm  commuicator /*in*/)
int MPI_Rsend(
    #指向包含消息的内存块的指针，如greeting
    void*  msg_buf_p /*in*/,
    #指定要发送的数据量，如strlen(greeting)+1
    int  msg_size  /*in*/,
    #数据类型MPI
    MPI_Datatype  msg_type /*in*/,
    #指定要接收消息的进程号
    int  dest /*in*/,
    #tag:0用于表示消息要打印的，为1表示消息用于计算的
    int tag /*in*/,
    #通信子
    MPI_Comm  commuicator /*in*/)
int MPI_Bsend(
    #函数MPI_Bsend使用的缓冲区必须通过调用该函数来指定
    #指向包含消息的内存块的指针，如greeting
    void*  msg_buf_p /*in*/,
    #指定要发送的数据量，如strlen(greeting)+1
    int  msg_size  /*in*/,
    #数据类型MPI
    MPI_Datatype  msg_type /*in*/,
    #指定要接收消息的进程号
    int  dest /*in*/,
    #tag:0用于表示消息要打印的，为1表示消息用于计算的
    int tag /*in*/,
    #通信子
    MPI_Comm  commuicator /*in*/)
```

指定缓冲空间

```js
int MPI_Buffer_attach(
    #指向用户程序分配的一块内存空间的指针
    void* buffer /*in*/,
    #这块空间以字节为单位的大小
    int buffer_size /*in*/)
int MPI_Buffer_detach(
    #返回的是之前分配的块的内存的地址
    void* buf_p /*out*/,
    #内存块的大小
    int* buffer_size_p /*out*/)
```

MPI_Pack_size  
1.被传送的数据需要的存储空间大小，可以通过调用MPI_Pack_size来计算。  
2.输出的参数给出了一条消息所需空间大小的上界。但仍然不够，一条消息还需要存储包括目的地，标志，通信子等信息，所以对于每条消息，都需要多余的开销。MPI针对这些增加的开销给出了上界常数：MPI_BSEND_OVERHEAD。

```js
int MPI_Pack_size(
    #元素个数
    int count /*in*/,
    #元素类型
    MPI_Datatype datatype /*in*/,
    #通信子
    MPI_Comm comm /*in*/,
    #计算的缓存区大小
    int* size_p /*out*/,
```

### 3.4 接收模式

MPI_Recv 进程接收消息  
MPI_Sendrecv 调用一次这个函数，分别执行一次阻塞式消息发送和一次消息接收。它的有用之处，实现了通信调度，使程序不再挂起和崩溃。  
MPI_Sendrecv_replace 如果发送和接收使用的是同一个缓冲区。

```js
int MPI_Recv(
    #指向内存块
    void*  msg_buf_p /*out*/,
    #指定内存块要存储对象的数据量
    int  buf_size  /*in*/,
    #对象的类型
    MPI_Datatype  buf_type /*in*/,
    #接受源
    int source /*in*/,
    #tag:0用于表示消息要打印的，为1表示消息用于计算的，要与发送的消息相匹配
    int tag /*in*/,
    #通信子
    MPI_Comm  commuicator /*in*/,
    #特殊MPI常量MPI_STATUS_IGNORE
    MPI_Status* status_p /*out*/)
int MPI_Sendrecv(
    #指向包含消息的内存块的指针
    void* send_buf_p /*in*/,
    #指定要发送的数据量
    int send_buf_size /*in*/,
    #数据类型MPI
    MPI_Datatype send_buf_type /*in*/,
    #指定要接收消息的进程号（可以与source相同也可以不相同，一般相同，代表向源发送，再向源接收）
    int dest /*in*/,
    #tag:0用于表示消息要打印的，为1表示消息用于计算的
    int send_tag /*in*/,
    #指向内存块
    void* recv_buf_p /*out*/,
    #指定内存块要存储对象的数据量
    int recv_buf_size /*in*/,
    #对象的类型
    MPI_Datatype recv_buf_type /*in*/,
    #接受源（可以与dest相同也可以不相同，一般相同，代表向源发送，再向源接收）
    int source /*in*/,
    #tag:0用于表示消息要打印的，为1表示消息用于计算的，要与发送的消息相匹配
    int recv_tag /*in*/,
    #通信子
    MPI_Comm  commuicator /*in*/,
    #特殊MPI常量MPI_STATUS_IGNORE
    MPI_Status* status_p /*in*/)
int MPI_Sendrecv_replace(
    #指向包含消息的内存块的指针
    void* buf_p /*in/out*/,
    #指定要发送/接收的数据量
    int buf_size /*in*/,
    #数据类型MPI
    MPI_Datatype buf_type /*in*/,
    #指定要接收消息的进程号（可以与source相同也可以不相同，一般相同，代表向源发送，再向源接收）
    int dest /*in*/,
    #tag:0用于表示消息要打印的，为1表示消息用于计算的，要与发送的消息相匹配
    int send_tag /*in*/,
    #指定要接收消息的进程号（可以与dest相同也可以不相同，一般相同，代表向源发送，再向源接收）
    int source /*in*/,
    #tag:0用于表示消息要打印的，为1表示消息用于计算的，要与发送的消息相匹配
    int recv_tag /*in*/,
    #通信子
    MPI_Comm  commuicator /*in*/,
    #特殊MPI常量MPI_STATUS_IGNORE
    MPI_Status* status_p /*in*/)
```

### 3.5 了解消息内容

```js
typedef struct MPI_Status {
    int count;
    int cancelled;
    int MPI_SOURCE;
    int MPI_TAG;
    int MPI_ERROR;
} MPI_Status;
#接收到的数据量不是存储在应用程序可以直接访问到的域中，但用户可以调用MPI_Get_count函数找回这个值
```

### 3.6 集合通信

以上为点对点通信，点对点通信函数通过标签和通信子匹配，对于集合通信来说，在通信子中，所有进程必须调用相同的集合通信函数。  
MPI_Reduce 树形结构通信  
MPI_Allreduce 蝶形结构通信  
MPI_Bcast  树形结构广播  
MPI_Scatter 散射  
MPI_Scatterv 不等长散射  
MPI_Gather 聚集  
MPI_Gatherv 不等长聚集  
MPI_Allgather 全局聚集  

```js
int MPI_Reduce(
    #发送缓冲区地址
    void*  input_data_p /*in*/,
    #接收缓冲区地址 
    void*  output_data_p /*out*/,
    #发送缓冲区内元素数
    int  count /*in*/,
    #发送元素的数据类型
    MPI_Datatype datatype /*in*/,
    #规约操作
    MPI_Op  operator /*in*/,
    #存放结果的根进程号
    int  dest_process /*in*/,
    #通信子
    MPI_Comm  comm /*in*/)
int MPI_Allreduce(
    #发送缓冲区地址 
    void*  input_data_p /*in*/,
    #接收缓冲区地址 
    void* output_data_p /*out*/,
    #发送缓冲区内元素数 
    int  count /*in*/,
    #发送元素的数据类型 
    MPI_Datatype  datatype /*in*/,
    #规约操作
    MPI_Op  operator /*in*/,
    #通信子
    MPI_Comm  comm /*in*/)
int MPI_Bcast(
    #接收缓冲区（已装载）
    void*  data_p /*in/out*/,
    #缓冲区内最大项数
    int  count /*in*/,
    #项的数据类型 
    MPI_Datatype  datatype /*in*/,
    #根进程序号 
    int  source /*in*/,
    #通信子
    MPI_Comm  comm /*in*/)
int MPI_Scatter(
    #发送缓冲区
    void* send_buf_p /*in*/,
    #发送到每个进程的数据量（一般和recv_count相同）
    int  send_count /*in*/,
    #发送元素的数据类型
    MPI_Datatype  send_type /*in*/,
    #接收缓冲区（已装载）
    void* recv_buf_p /*out*/,
    #每次接收的元素数
    int   recv_count /*in*/,
    #接收元素的数据类型
    MPI_Datatype  recv_type /*in*/,
    #接收进程序号
    int  src_proc /*in*/,
    #通信子
    MPI_Comm  comm /*in*/)
int MPI_Scatterv(
    #发送的内存地址
    void* sendbuf /*in*/,
    #发送sendcount[q]是发送q的sendtype类型的对象的数量
    int* sendcounts /*in*/,
    #偏移量的起始位置sendbuf+displacements[q]
    int* displacements /*in*/,
    #数据类型
    MPI_Datatype sendtype /*in*/,
    #接收的数据缓存块
    void* recvbuf /*out*/,
    #接收的个数
    int recvcount /*in*/,
    #发送根目录
    int root /*in*/,
    #通信子
    MPI_Comm comm /*in*/)
int MPI_Gather(
    #发送缓冲区
    void* send_buf_p /*in*/,
    #发送缓冲区内元素数
    int  send_count /*in*/,
    #发送元素的数据类型
    MPI_Datatype  send_type /*in*/,
    #接收缓冲区（已装载）
    void* recv_buf_p /*out*/,
    #从每个进程中接收到的数据量（一般和send_count相同）
    int   recv_count /*in*/,
    #接收元素的数据类型
    MPI_Datatype  recv_type /*in*/,
    #接收进程序号
    int  dest_proc /*in*/,
    #通信子
    MPI_Comm  comm /*in*/)
int MPI_Gatherv(
    #发送的内存地址
    void* sendbuf /*in*/,
    #发送对应内存的元素个数
    int sendcounts /*in*/,
    #数据类型
    MPI_Datatype sendtype /*in*/,
    #接收的数据缓存块
    void* recvbuf /*out*/,
    #接收recvcounts[q]是发送q的sendtype类型的对象的数量
    int* recvcounts /*in*/,
    #偏移量的起始位置recvbuf+displacements[q]
    int* displacements /*in*/,
    #接收数据的根目录
    int root /*in*/,
    #通信子
    MPI_Comm comm /*in*/)
int MPI_Allgather(
    #发送缓冲区
    void* send_buf_p /*in*/,
    #发送缓冲区内元素数
    int  send_count /*in*/,
    #发送元素的数据类型
    MPI_Datatype  send_type /*in*/,
    #接收缓冲区（已装载）
    void* recv_buf_p /*out*/,
    #从每个进程中接收到的数据量（一般和send_count相同）
    int   recv_count /*in*/,
    #接收元素的数据类型
    MPI_Datatype  recv_type /*in*/,
    #通信子
    MPI_Comm  comm /*in*/)
```

### 3.7 MPI派生数据类型

如果发送数据的函数知道数据项的类型以及在内存中数据项集合的相对位置，就可以在数据项被发送出去之前将数据项聚集起来，就会减少程序运行时间。MPI提供了三个基本手段：不同通信函数中 count参数、派生数据类型和MPI_Pack/Unpack函数。  
MPI_Get_address  
MPI_Type_create_struct 创建由不同基本数据类型的元素所组成的派生数据类型  
MPI_Type_commit 允许MPI实现为了在通信函数内使用这一数据类型（例：input_mpi_t）  
MPI_Type_free 当我们使用新的数据类型时，可以调用一个函数去释放额外的存储空间  
MPI_Type_create_resized 创建一个能在数据类型上缓冲的新属性类型  

```js
int MPI_Get_address(
    #所指向的内存单元地址
    void* location_p /*in*/,
    #特殊类型MPI_Aint，长度足以表示系统地址
    MPI_Aint* address_p /*out*/)
int MPI_Type_create_struct(
    #数据类型中元素个数（例：double,double,int = 3）
    int  count /*in*/,
    #每个数据项的元素个数（可以是数组）（例：{1,1,1}）
    int  array_of_blocklengths[] /*in*/,
    #每个数据项距离消息起始位置的偏移量（例：{0,16,24}）
    MPI_Aint  array_of_displacements[] /*in*/,
    #每个数据项的类型（例：{MPI_DOUBLE,MPI_DOUBLE,MPI_INT}）
    MPI_Datatype  array_of_types[] /*in*/,
    #自定义数据类型出口（例：MPI_Datatype input_mpi_t）
    MPI_Datatype*  new_type_p /*out*/)
int MPI_Type_commit(
    #新构建的数据类型（例：MPI_Datatype input_mpi_t）
    MPI_Datatype* new_mpi_t_p /*in/out*/)
int MPI_Type_free(
    #已构建的数据类型（例：MPI_Datatype input_mpi_t）
    MPI_Datatype* old_mpi_t_p /*in/out*/)
int MPI_Type_create_resized(
        #旧数据类型
        MPI_Datatype oldtype,  
        #下限
        MPI_Aint     lb,
        #宽度
        MPI_Aint     extent,
        #新类型
        MPI_Datatype *newtype)
```

### 3.8 MPI程序评估

MPI_Wtime() 时钟计时  
MPI_Barrier 能够确保同一通信子所有进程都完成调用该函数之前，没有进程能够提前返回  
MPI_Scan 计算前缀和，分配到各个节点  
MPI_Type_contiguous 从数组中收集邻接元素，然后创建派生数据类型  
MPI_Type_vector 可以用来将数组中的数据块组合起来构建派生数据类型，这些块大小相同，在数组中的间隔是等距的  
MPI_Pack 每次复制一块要发送的数据到用户提供的缓冲区，该缓冲区既可以接收数据或者发送数据  
MPI_Unpack 将缓冲区的数据解包 PS：除了进程0以外，其他进程都可以用MPI_Unpack来解包数据  
MPI_Iprobe 只测试是否有消息可用，不会真地接收消息  

```js
#返回从过去某一个时刻开始所经过的秒数,墙上时钟
double MPI_Wtime(void)

                         #通信子
int MPI_Barrier(MPI_Comm comm /*int*/)
int MPI_Scan(
    #发送缓冲区地址
    void* sendbuf_p /*int*/,
    #接收缓冲区地址
    void* recvbuf_p /*out*/,
    #sendbuf_p和recvbuf_p都应该指向有count个datatype类型元素的数据块
    int count /*int*/,
    #发送元素的数据类型
    MPI_Datatype datatype /*int*/,
    #规约操作
    MPI_Op op /*int*/,
    #通信子
    MPI_Comm comm /*int*/)
int MPI_Type_contiguous(
    #复制旧数据类型次数
    int count /*int*/,
    #原来数据类型
    MPI_Datatype old_mpi_t /*int*/,
    #新数据类型
    MPI_Datatype* new_mpi_t_p /*out*/)
int MPI_Type_vector(
    #块的个数
    int count /*in*/,
    #块的大小
    int blocklength /*in*/,
    #块与块的偏移量
    int stride /*in*/,
    #旧MPI数据类型
    MPI_Datatype old_mpi_t /*in*/,
    #新MPI数据类型
    MPI_Datatype* new_mpi_t_p /*out*/)
int MPI_Type_indexed(
    #数据类型中元素个数
    int count; /*in*/,
    #每个数据项的元素个数（可以是数组）
    int  array_of_blocklengths[] /*in*/,
    #每个数据项距离消息起始位置的偏移量
    int  array_of_displacements /*in*/,
    #旧数据类型
    MPI_Datatype old_mpi_t /*in*/,
    #自定义数据类型出口
    MPI_Datatype*  new_type_p /*out*/)
int MPI_Pack(
    #放入存储区的内存地址
    void* in_buf /*in*/,
    #放入存储区的个数
    int in_buf_count /*in*/,
    #元素的数据类型
    MPI_Datatype datatype /*in*/,
    #放入的数据包
    void* pack_buf /*in*/,
    #数据包大小
    int pack_buf_sz /*in*/,
    #关键参数，当调用MPI_Pack时，该参数指向pack_buf中第一个可访问元素的位置
    int* position_p /*in/out*/,
    #通信子
    MPI_Comm comm /*int*/)
int MPI_Unpack(
    #数据包内存地址
    void* pack_buf /*in*/,
    #数据包大小
    int pack_buf_sz /*in*/,
    #关键参数，当调用MPI_Unpack时，该参数指向pack_buf中第一个可访问元素的位置
    int* position_p /*in/out*/,
    #放入数据的内存地址
    void* out_buf /*in*/,
    #放入数据的内存大小
    int out_buf_count /*in*/,
    #元素的数据类型
    MPI_Datatype datatype /*in*/,
    #通信子
    MPI_Comm comm /*int*/)
int MPI_Iprobe(
    #来源进程，可以选用 MPI_ANY_SOURCE
    int source /*in*/,
    #tag:0用于表示消息要打印的，为1表示消息用于计算的。可以选用 MPI_ANY_TAG
    int tag /*in*/,
    #通信子
    MPI Comm comm /*in*/,
    #标记为tag的消息是否可用，可以则为true。否则为false
    int* msg_avail_p /*out*/,
    #特殊MPI常量，例如将status_p->MPI_SOURCE赋予接收到消息来源的线程号。
    MPI_Status* status_p /*out*/)
```
